{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca5c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021ff832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain_pinecone import Pinecone as LangchainPinecone\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e327a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucess\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "print(\"sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5ae7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to AWS Bedrock in region: None...\n"
     ]
    }
   ],
   "source": [
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    region_name=os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    ")\n",
    "\n",
    "bedrock_client = boto3_session.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "print(f\"Connecting to AWS Bedrock in region: {os.getenv('AWS_DEFAULT_REGION')}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ea26bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AWS Bedrock models initialized!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\", \n",
    "    client=bedrock_client, \n",
    "    model_kwargs={\"temperature\": 0.1, \"max_tokens\": 512} \n",
    ")\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "print(\"‚úÖ AWS Bedrock models initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56832e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming the world. Machine Learning is a subset of AI \n",
    "that enables systems to learn from data. Deep Learning uses neural networks with multiple layers \n",
    "to process complex patterns. Natural Language Processing (NLP) helps machines understand human language.\n",
    "Large Language Models like GPT and Llama are revolutionizing how we interact with AI systems.\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval and generation for better responses.\n",
    "Vector databases store embeddings for efficient similarity search in AI applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acd7c16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into 3 chunks \n",
      "--- chunk 1 --- \n",
      "\n",
      " Artificial Intelligence (AI) is transforming the world. Machine Learning is a subset of AI \n",
      "that enables systems to learn from data. Deep Learning uses neural networks with multiple layers \n",
      "\n",
      "--- chunk 2 --- \n",
      "\n",
      " to process complex patterns. Natural Language Processing (NLP) helps machines understand human language.\n",
      "Large Language Models like GPT and Llama are revolutionizing how we interact with AI systems. \n",
      "\n",
      "--- chunk 3 --- \n",
      "\n",
      " Retrieval-Augmented Generation (RAG) combines retrieval and generation for better responses.\n",
      "Vector databases store embeddings for efficient similarity search in AI applications. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 50,\n",
    "    separators=[\"\\n\\n\", \"\\n\",\".\",\" \",\"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(data)\n",
    "print(f\"Data split into {len(chunks)} chunks \")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- chunk {i+1} --- \\n\\n {chunk} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7484be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone index create successfully\n"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"pash-index-2\"\n",
    "\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")\n",
    "        \n",
    "    )\n",
    "    print(\"pinecone index create successfully\")\n",
    "else:\n",
    "    print(\"existing pinecone available\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba58df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document embedded and store in pinecone\n"
     ]
    }
   ],
   "source": [
    "vectorstore = LangchainPinecone.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(\"document embedded and store in pinecone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f24443",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is for FAISS vector store\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Documents embedded and stored in FAISS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4371d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98d8beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "format_docs_runnable = RunnableLambda(format_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "424ab296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert Insurance Assistant. Use the following pieces of retrieved context to answer the question.\n",
    "If the answer is not in the context, just say that you don't know. Do not try to make up an answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ef9fb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain built using LCEL!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        context=retriever | format_docs_runnable,\n",
    "        question=RunnablePassthrough()\n",
    "    )\n",
    "    | prompt          # ‚úÖ PromptTemplate, NOT string\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain built using LCEL!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10fa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ecd2ef02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Question: What is RAG and how does it work?\n",
      "\n",
      "üí° Answer:\n",
      "Based on the provided context, RAG (Retrieval-Augmented Generation) is a technique that combines retrieval and generation to improve AI responses. The context indicates that RAG helps in generating more accurate and contextually relevant answers by first retrieving relevant information before generating a response.\n",
      "\n",
      "While the context provides a basic definition, it doesn't go into extensive detail about the specific mechanics of how RAG works. The context suggests that it involves using techniques like vector databases to store and efficiently search embeddings, which can help in retrieving relevant information quickly.\n",
      "\n",
      "The context also mentions related AI technologies like Natural Language Processing (NLP), Large Language Models, and Machine Learning, which are likely components that support RAG's functionality, but doesn't explicitly explain RAG's full operational process.\n",
      "\n",
      "So in summary, RAG is a method that enhances AI response generation by first retrieving relevant information before producing an answer, but the specific technical details of its operation are not fully elaborated in this context.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"What is RAG and how does it work?\"\n",
    "\n",
    "print(f\"\\nüîç Question: {test_question}\\n\")\n",
    "response = rag_chain.invoke(test_question)\n",
    "print(f\"üí° Answer:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a7292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pashdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

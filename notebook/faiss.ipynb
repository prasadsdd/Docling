{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a5bd13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\pashdoc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    "    RunnableLambda\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "print(\"‚úÖ Environment loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b43924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to AWS Bedrock in region: None\n"
     ]
    }
   ],
   "source": [
    "boto3_session = boto3.Session(\n",
    "    aws_access_key_id=os.getenv(\"AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n",
    "    region_name=os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n",
    ")\n",
    "\n",
    "bedrock_client = boto3_session.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "print(f\"Connecting to AWS Bedrock in region: {os.getenv('AWS_DEFAULT_REGION')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459b58a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bedrock LLM & Embeddings initialized\n"
     ]
    }
   ],
   "source": [
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    client=bedrock_client,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Bedrock LLM & Embeddings initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd68fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "Artificial Intelligence (AI) is transforming the world. Machine Learning is a subset of AI \n",
    "that enables systems to learn from data. Deep Learning uses neural networks with multiple layers \n",
    "to process complex patterns. Natural Language Processing (NLP) helps machines understand human language.\n",
    "Large Language Models like GPT and Llama are revolutionizing how we interact with AI systems.\n",
    "Retrieval-Augmented Generation (RAG) combines retrieval and generation for better responses.\n",
    "Vector databases store embeddings for efficient similarity search in AI applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2731b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data split into 3 chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(data)\n",
    "\n",
    "print(f\"‚úÖ Data split into {len(chunks)} chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f00728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Documents embedded and stored in FAISS\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_texts(\n",
    "    texts=chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Documents embedded and stored in FAISS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1809793e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c00bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "format_docs_runnable = RunnableLambda(format_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0087da74",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "You are an expert Insurance Assistant. Use the following pieces of retrieved context to answer the question.\n",
    "If the answer is not in the context, just say that you don't know. Do not try to make up an answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14c68ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain built using LCEL + FAISS\n"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        context=retriever | format_docs_runnable,\n",
    "        question=RunnablePassthrough()\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain built using LCEL + FAISS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6bcd7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Question: What is RAG and how does it work?\n",
      "\n",
      "üí° Answer:\n",
      "Based on the provided context, RAG (Retrieval-Augmented Generation) is a technique that combines retrieval and generation to improve AI responses. The context indicates that RAG involves using retrieval mechanisms, likely with vector databases that store embeddings, to enhance the generation of responses. \n",
      "\n",
      "However, the context does not provide a detailed explanation of exactly how RAG works. While it mentions some related concepts like vector databases and AI technologies, the specific mechanics of RAG are not fully elaborated in this context.\n",
      "\n",
      "So my most accurate response is: RAG combines retrieval and generation techniques to create better AI responses, but I don't have enough detailed information from the context to explain precisely how it works.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"What is RAG and how does it work?\"\n",
    "\n",
    "print(f\"\\nüîç Question: {test_question}\\n\")\n",
    "response = rag_chain.invoke(test_question)\n",
    "print(f\"üí° Answer:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be98ba4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‚ö†Ô∏è Optional (VERY IMPORTANT)\n",
    "\n",
    "#If you want persistence (store FAISS index on disk):\n",
    "\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "\n",
    "#Load later:\n",
    "\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pashdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

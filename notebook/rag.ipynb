{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ee206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2723926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\pashdoc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded. Environment verified.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Environment Setup\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain & AI Libraries\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "# Pinecone & Evaluation\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.run_config import RunConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load Environment Variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded. Environment verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78b159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index 'sbi-home-insurance-rag-hybrid' already exists.\n",
      "üîç Checking if 'SBIhomeinsurance_home.pdf' is already in the database...\n",
      "‚úÖ File 'SBIhomeinsurance_home.pdf' detected in Pinecone.\n",
      "üöÄ SKIPPING Docling & Embeddings to save cost.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Smart Initialization & Duplicate Check\n",
    "# Configuration\n",
    "file_path = \"SBIhomeinsurance_home.pdf\" # Make sure this matches your file name\n",
    "index_name = \"sbi-home-insurance-rag-hybrid\" # Using your existing hybrid index name\n",
    "\n",
    "# 1. Connect to Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# 2. Check if Index Exists\n",
    "existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    print(f\"‚ö†Ô∏è Index '{index_name}' not found. Creating it...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024, # Titan v2\n",
    "        metric=\"dotproduct\", # Required for Hybrid\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    time.sleep(20) # Wait for init\n",
    "    print(\"‚úÖ Index created successfully.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Index '{index_name}' already exists.\")\n",
    "\n",
    "# 3. Connect to the Index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# 4. Check if File is Already Ingested (The \"Smart\" Check)\n",
    "# We perform a dummy query filtering by this specific source file\n",
    "print(f\"üîç Checking if '{file_path}' is already in the database...\")\n",
    "\n",
    "# We use a dummy vector just to trigger the metadata filter\n",
    "dummy_vector = [0.0] * 1024 \n",
    "check_response = index.query(\n",
    "    vector=dummy_vector,\n",
    "    top_k=1,\n",
    "    filter={\"source\": file_path},\n",
    "    include_metadata=False\n",
    ")\n",
    "\n",
    "if len(check_response['matches']) > 0:\n",
    "    print(f\"‚úÖ File '{file_path}' detected in Pinecone.\")\n",
    "    print(\"üöÄ SKIPPING Docling & Embeddings to save cost.\")\n",
    "    should_ingest = False\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è File '{file_path}' NOT found in Pinecone.\")\n",
    "    print(\"‚öôÔ∏è Proceeding with Ingestion...\")\n",
    "    should_ingest = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87a8f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipping Loading & Chunking (Data already exists).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load & Chunk (Conditional)\n",
    "final_chunks = []\n",
    "\n",
    "if should_ingest:\n",
    "    print(f\"üìÑ Starting Docling processing for {file_path}...\")\n",
    "    \n",
    "    # A. Load with Docling (Export to Markdown)\n",
    "    loader = DoclingLoader(\n",
    "        file_path=file_path,\n",
    "        export_type=ExportType.MARKDOWN\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(\"‚úÖ PDF Loaded via Docling.\")\n",
    "\n",
    "    # B. Split by Headers (Level 1)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_header_splits = markdown_splitter.split_text(docs[0].page_content)\n",
    "    \n",
    "    # C. Split by Size (Level 2)\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 200\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "    # D. Add Metadata Tags (Crucial for Smart Indexing)\n",
    "    for chunk in final_chunks:\n",
    "        chunk.metadata[\"source\"] = file_path # Used for filtering later\n",
    "        # We also keep the 'text' in metadata for Hybrid retrieval\n",
    "        chunk.metadata[\"text\"] = chunk.page_content \n",
    "    \n",
    "    print(f\"‚úÖ Chunking Complete. Created {len(final_chunks)} chunks.\")\n",
    "    print(\"Sample Metadata:\", final_chunks[0].metadata)\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Loading & Chunking (Data already exists).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cb347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "060b3122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 23:31:10,834 - INFO - Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipped Ingestion. Loaded existing BM25 params from file.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Hybrid Embedding & Upsert (Conditional)\n",
    "import boto3\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "# 1. Initialize AWS Bedrock Embeddings (Need this for both Ingestion AND Querying)\n",
    "boto3_session = boto3.Session()\n",
    "bedrock_client = boto3_session.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "# 2. Initialize BM25 Encoder\n",
    "bm25 = BM25Encoder()\n",
    "bm25_filename = \"bm25_values.json\"\n",
    "\n",
    "if should_ingest:\n",
    "    print(\"‚öôÔ∏è Generatings Embeddings & Upserting...\")\n",
    "    \n",
    "    # A. Fit BM25 on the new text\n",
    "    chunk_texts = [chunk.page_content for chunk in final_chunks]\n",
    "    bm25.fit(chunk_texts)\n",
    "    bm25.dump(bm25_filename) # Save for future use\n",
    "    print(\"‚úÖ BM25 Encoder fitted and saved.\")\n",
    "    \n",
    "    # B. Generate Vectors & Upsert\n",
    "    vectors_to_upsert = []\n",
    "    \n",
    "    print(f\"Generating vectors for {len(final_chunks)} chunks...\")\n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        # 1. Dense Vector (Titan)\n",
    "        dense_vec = embeddings.embed_query(chunk.page_content)\n",
    "        \n",
    "        # 2. Sparse Vector (BM25)\n",
    "        sparse_vec = bm25.encode_documents(chunk.page_content)\n",
    "        \n",
    "        # 3. Create ID (Unique based on source + index)\n",
    "        # We use a simple hash or index. Here index 'i' is fine for this run.\n",
    "        # Ideally, hash the text to avoid dupes, but for now:\n",
    "        vector_id = f\"{file_path}_{i}\"\n",
    "        \n",
    "        vectors_to_upsert.append({\n",
    "            \"id\": vector_id,\n",
    "            \"values\": dense_vec,\n",
    "            \"sparse_values\": sparse_vec,\n",
    "            \"metadata\": chunk.metadata # Includes 'source' and 'text'\n",
    "        })\n",
    "        \n",
    "    # C. Batch Upsert to Pinecone\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(vectors_to_upsert), batch_size):\n",
    "        batch = vectors_to_upsert[i : i + batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "        print(f\"   Uploaded batch {i} to {i+batch_size}\")\n",
    "        \n",
    "    print(\"‚úÖ Ingestion Complete.\")\n",
    "\n",
    "else:\n",
    "    # If we skipped ingestion, we MUST load the BM25 model from disk\n",
    "    # so we can still run queries.\n",
    "    if os.path.exists(bm25_filename):\n",
    "        bm25.load(bm25_filename)\n",
    "        print(\"‚úÖ Skipped Ingestion. Loaded existing BM25 params from file.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: BM25 file not found. You might need to re-ingest if retrieval fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0b45b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 23:31:26,790 - INFO - Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cohere Re-ranker Initialized.\n",
      "‚úÖ Retrieval Logic Defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Setup Retrieval & Re-ranking Engines\n",
    "from typing import List\n",
    "\n",
    "# 1. Define the Bedrock Cohere Re-ranker Class\n",
    "class BedrockCohereReranker:\n",
    "    def __init__(self, region_name=\"us-east-1\"):\n",
    "        self.client = boto3.client(\"bedrock-runtime\", region_name=region_name)\n",
    "        self.model_id = \"cohere.rerank-v3-5:0\"\n",
    "\n",
    "    def rerank(self, query: str, docs: List[str], top_n: int = 5):\n",
    "        # Docs must be a list of strings for the API\n",
    "        if not docs: return []\n",
    "        \n",
    "        request_body = {\n",
    "            \"query\": query, \n",
    "            \"documents\": docs, \n",
    "            \"top_n\": top_n, \n",
    "            \"api_version\": 2\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.client.invoke_model(modelId=self.model_id, body=json.dumps(request_body))\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            results = response_body.get(\"results\", [])\n",
    "            return results # Returns list of {'index': int, 'relevance_score': float}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Rerank Error: {e}\")\n",
    "            # Fallback: return indices 0..top_n\n",
    "            return [{\"index\": i, \"relevance_score\": 0.0} for i in range(min(len(docs), top_n))]\n",
    "\n",
    "# Initialize the Reranker\n",
    "reranker = BedrockCohereReranker()\n",
    "print(\"‚úÖ Cohere Re-ranker Initialized.\")\n",
    "\n",
    "# 2. Define the \"Intelligent Retrieval\" Function\n",
    "# This combines Hybrid Search (Pinecone) + Re-ranking (Cohere)\n",
    "def intelligent_retrieval(query: str) -> str:\n",
    "    print(f\"üîé Searching for: '{query}'\")\n",
    "    \n",
    "    # A. Hybrid Search in Pinecone (Top 25)\n",
    "    dense_vec = embeddings.embed_query(query)\n",
    "    # Note: If you want strict keyword matching, enable the line below:\n",
    "    # sparse_vec = bm25.encode_queries(query) \n",
    "    \n",
    "    results = index.query(\n",
    "        vector=dense_vec,\n",
    "        # sparse_vector=sparse_vec, # Uncomment if passing sparse values\n",
    "        top_k=25,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Extract just the text from the matches\n",
    "    raw_docs = [match['metadata']['text'] for match in results['matches']]\n",
    "    \n",
    "    if not raw_docs:\n",
    "        return \"\"\n",
    "\n",
    "    # B. Re-ranking (Filter 25 -> Top 5)\n",
    "    rerank_results = reranker.rerank(query, raw_docs, top_n=5)\n",
    "    \n",
    "    # C. Format the Top 5 for the LLM\n",
    "    top_docs_text = []\n",
    "    for res in rerank_results:\n",
    "        idx = res['index']\n",
    "        top_docs_text.append(raw_docs[idx])\n",
    "        \n",
    "    return \"\\n\\n\".join(top_docs_text)\n",
    "\n",
    "print(\"‚úÖ Retrieval Logic Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b027279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ anthropic.claude-3-5-haiku Model Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5: Initialize anthropic.claude-3-5-haiku Model\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# We use the US Cross-Region Inference Profile for Llama 3.1\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",  ## us.meta.llama3-1-70b-instruct-v1:0\n",
    "    client=bedrock_client, # We defined this client in Cell 4\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_tokens\": 512} # max_tokens\": 2048\n",
    ")\n",
    "\n",
    "print(\"‚úÖ anthropic.claude-3-5-haiku Model Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35d0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c10ff8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Chain (Production Ready) Created.\n",
      "\n",
      "üß™ Sanity Check Query: 'What specific exclusions apply to loss caused by Subsidence?'\n",
      "----------------------------------------\n",
      "üîé Searching for: 'What specific exclusions apply to loss caused by Subsidence?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 00:01:54,959 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '6da7269b-ae93-4dbb-8bac-fd18ea368bfe', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 18:31:55 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': '6da7269b-ae93-4dbb-8bac-fd18ea368bfe', 'x-amzn-bedrock-invocation-latency': '87', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-11 00:01:57,215 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, for subsidence, the following exclusions apply:\n",
      "\n",
      "Subsidence is excluded when caused by:\n",
      "a. Normal cracking, settlement or bedding down of new structures\n",
      "b. The settlement or movement of made up ground\n",
      "c. Coastal or river erosion\n",
      "d. Defective design or workmanship or use of defective materials\n",
      "e. Demolition, construction, structural alterations or repair of any property\n",
      "f. Groundworks or excavations\n",
      "\n",
      "These exclusions are specifically listed under section 6 of the context, which covers \"Subsidence of the land on which Your Home Buildings stands, Landslide, Rockslide\".\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: LLM Chain Setup\n",
    "# 1. Define the Prompt\n",
    "# We strictly tell the LLM to use ONLY the provided context.\n",
    "prompt_template = \"\"\"\n",
    "You are an expert Insurance Assistant. Use the following pieces of retrieved context to answer the question.\n",
    "If the answer is not in the context, just say that you don't know. Do not try to make up an answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 2. Define the Chain\n",
    "# This pipeline does: Take Query -> Get Smart Context -> Format Prompt -> Run Llama 3 -> Parse String\n",
    "rag_chain_final = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(intelligent_retrieval), # Uses our Hybrid + Rerank function\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain (Production Ready) Created.\")\n",
    "\n",
    "# 3. Quick Sanity Check\n",
    "# Let's run a simple test to make sure the chain flows correctly\n",
    "\n",
    "test_q = \"What specific exclusions apply to loss caused by Subsidence?\"\n",
    "#\"What is the deductible for Personal Property?\"\n",
    "#\"what is the name of company which provides this insurance? and give me address for this company. also give me contact details for this comapny\"\n",
    "#\"from the document tell me in terms of payment what policys provide how much insurance back means in terms of money\"\n",
    "\n",
    "\n",
    "print(f\"\\nüß™ Sanity Check Query: '{test_q}'\")\n",
    "print(\"-\" * 40)\n",
    "print(rag_chain_final.invoke(test_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94131bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Final Evaluation on Test Set...\n",
      "--------------------------------------------------\n",
      "Asking: What specific exclusions apply to loss caused by Subsidence?\n",
      "üîé Searching for: 'What specific exclusions apply to loss caused by Subsidence?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:30:42,721 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '80e646a4-7d00-42c3-b10f-33d176fed6c3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:00:23 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': '80e646a4-7d00-42c3-b10f-33d176fed6c3', 'x-amzn-bedrock-invocation-latency': '95', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:31:17,781 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking: What is the deductible for Personal Property?\n",
      "üîé Searching for: 'What is the deductible for Personal Property?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:31:27,586 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '0574e333-07bf-451a-a7a8-964937c2869c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:01:22 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': '0574e333-07bf-451a-a7a8-964937c2869c', 'x-amzn-bedrock-invocation-latency': '78', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:31:39,071 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking: What are the specific exclusions for Riot, strikes, or malicious damages?\n",
      "üîé Searching for: 'What are the specific exclusions for Riot, strikes, or malicious damages?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:31:51,091 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '5f507c34-7f0a-4b1f-896b-f02b37e2ddee', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:01:42 GMT', 'content-type': 'application/json', 'content-length': '43437', 'connection': 'keep-alive', 'x-amzn-requestid': '5f507c34-7f0a-4b1f-896b-f02b37e2ddee', 'x-amzn-bedrock-invocation-latency': '73', 'x-amzn-bedrock-input-token-count': '16'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:32:01,671 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üèÜ FINAL PROJECT ACCURACY REPORT\n",
      "============================================================\n",
      "\n",
      "Q1: What specific exclusions apply to loss caused by Subsidence?\n",
      "A: According to the context, the specific exclusions that apply to loss caused by Subsidence of the land on which the home building stands are:\n",
      "\n",
      "a. normal cracking, settlement or bedding down of new structures,\n",
      "b. the settlement or movement of made up ground,\n",
      "c. coastal or river erosion,\n",
      "d. defective design or workmanship or use of defective materials, or demolition, construction, structural alterations or repair of any property, or groundworks or excavations.\n",
      "----------------------------------------\n",
      "\n",
      "Q2: What is the deductible for Personal Property?\n",
      "A: The deductible for Personal Property is not explicitly mentioned in the context. However, it does mention deductibles for Jewellery & Valuables (5% of the claim amount subject to a minimum of Rs 2500) and portable equipment's (5% of claim amount subject to a minimum of Rs 1000).\n",
      "----------------------------------------\n",
      "\n",
      "Q3: What are the specific exclusions for Riot, strikes, or malicious damages?\n",
      "A: According to the context, the specific exclusions for Riot, Strikes, or Malicious Damages are:\n",
      "\n",
      "* Caused by temporary or permanent dispossession, confiscation, commandeering, requisition or destruction by order of the government or any lawful authority.\n",
      "* Caused by temporary or permanent dispossession of Your Home by unlawful occupation by any person.\n",
      "\n",
      "(Section 10 of the context)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Final Evaluation Run\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the Hard Questions\n",
    "test_questions = [\n",
    "    \"What specific exclusions apply to loss caused by Subsidence?\", \n",
    "    \"What is the deductible for Personal Property?\",\n",
    "    \"What are the specific exclusions for Riot, strikes, or malicious damages?\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Running Final Evaluation on Test Set...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results = []\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"Asking: {q}\")\n",
    "    try:\n",
    "        # Run the robust chain\n",
    "        answer = rag_chain_final.invoke(q)\n",
    "        \n",
    "        # Save result\n",
    "        results.append({\n",
    "            \"Question\": q,\n",
    "            \"AI Answer\": answer.strip(),\n",
    "            \"Status\": \"‚úÖ Success\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"Question\": q,\n",
    "            \"AI Answer\": f\"ERROR: {e}\",\n",
    "            \"Status\": \"‚ùå Failed\"\n",
    "        })\n",
    "\n",
    "# 2. Display Results in a Clean Table\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ FINAL PROJECT ACCURACY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print full details for verification\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"\\nQ{i+1}: {row['Question']}\")\n",
    "    print(f\"A: {row['AI Answer']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367fc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580cb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching contexts for evaluation...\n",
      "üîé Searching for: 'What specific exclusions apply to loss caused by Subsidence?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:36:10,511 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '9bbd495c-b7a9-4f53-8227-d8358966bec3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:05:29 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': '9bbd495c-b7a9-4f53-8227-d8358966bec3', 'x-amzn-bedrock-invocation-latency': '79', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is the deductible for Personal Property?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:36:20,926 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '7df64f13-5668-44b7-aadc-8bc414676274', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:06:17 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': '7df64f13-5668-44b7-aadc-8bc414676274', 'x-amzn-bedrock-invocation-latency': '84', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What are the specific exclusions for Riot, strikes, or malicious damages?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:36:27,671 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '45222c46-ecb4-40e1-b43b-e07e1dcd5813', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:06:26 GMT', 'content-type': 'application/json', 'content-length': '43437', 'connection': 'keep-alive', 'x-amzn-requestid': '45222c46-ecb4-40e1-b43b-e07e1dcd5813', 'x-amzn-bedrock-invocation-latency': '72', 'x-amzn-bedrock-input-token-count': '16'}, 'RetryAttempts': 0}\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1600\\1291553097.py:38: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  ragas_llm = LangchainLLMWrapper(llm)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1600\\1291553097.py:39: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë®‚Äç‚öñÔ∏è Calculating Final Scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]2025-12-06 16:36:34,386 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:36:53,511 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:13,642 - ERROR - Exception raised in Job[0]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Evaluating:  11%|‚ñà         | 1/9 [00:39<05:16, 39.51s/it]2025-12-06 16:37:13,691 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:13,696 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:13,701 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:42,161 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '05eef54a-6587-4f86-a5e2-7e66d57a2745', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:07:41 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': '05eef54a-6587-4f86-a5e2-7e66d57a2745', 'x-amzn-bedrock-invocation-latency': '75', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:38:43,652 - ERROR - Error raised by inference endpoint\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 892, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1017, in _raw_read\n",
      "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1000, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 482, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 631, in _safe_read\n",
      "    data = self.fp.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1307, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1163, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 98, in read\n",
      "    chunk = self._raw_stream.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1101, in read\n",
      "    data = self._raw_read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1016, in _raw_read\n",
      "    with self._error_catcher():\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 897, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\n",
      "urllib3.exceptions.ReadTimeoutError: AWSHTTPSConnectionPool(host='bedrock-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\langchain_aws\\embeddings\\bedrock.py\", line 258, in _invoke_model\n",
      "    response_body = json.loads(response.get(\"body\").read())\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 101, in read\n",
      "    raise ReadTimeoutError(endpoint_url=e.url, error=e)\n",
      "botocore.exceptions.ReadTimeoutError: Read timeout on endpoint URL: \"None\"\n",
      "2025-12-06 16:38:43,951 - ERROR - Exception raised in Job[1]: ReadTimeoutError(Read timeout on endpoint URL: \"None\")\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 2/9 [02:09<08:05, 69.39s/it]2025-12-06 16:38:43,961 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [02:16<04:04, 40.72s/it]2025-12-06 16:38:50,561 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:05,641 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:17,491 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [02:59<03:27, 41.49s/it]2025-12-06 16:39:33,251 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:33,261 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:33,266 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:43,061 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '65b1c2ee-9dfb-4b30-b0af-c2870038a07b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:09:38 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': '65b1c2ee-9dfb-4b30-b0af-c2870038a07b', 'x-amzn-bedrock-invocation-latency': '79', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:40:02,221 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'dd4367a8-f380-451a-9d8a-0a4540faa367', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:09:56 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': 'dd4367a8-f380-451a-9d8a-0a4540faa367', 'x-amzn-bedrock-invocation-latency': '73', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:02,231 - ERROR - Error raised by inference endpoint\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 892, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1017, in _raw_read\n",
      "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1000, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 482, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 631, in _safe_read\n",
      "    data = self.fp.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1307, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1163, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 98, in read\n",
      "    chunk = self._raw_stream.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1101, in read\n",
      "    data = self._raw_read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1016, in _raw_read\n",
      "    with self._error_catcher():\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 897, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\n",
      "urllib3.exceptions.ReadTimeoutError: AWSHTTPSConnectionPool(host='bedrock-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\langchain_aws\\embeddings\\bedrock.py\", line 258, in _invoke_model\n",
      "    response_body = json.loads(response.get(\"body\").read())\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 101, in read\n",
      "    raise ReadTimeoutError(endpoint_url=e.url, error=e)\n",
      "botocore.exceptions.ReadTimeoutError: Read timeout on endpoint URL: \"None\"\n",
      "2025-12-06 16:41:02,231 - ERROR - Exception raised in Job[4]: ReadTimeoutError(Read timeout on endpoint URL: \"None\")\n",
      "Evaluating:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [04:28<03:54, 58.63s/it]2025-12-06 16:41:02,256 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [04:35<02:03, 41.31s/it]2025-12-06 16:41:09,961 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:20,156 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [05:01<01:12, 36.10s/it]2025-12-06 16:41:35,311 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:35,311 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:35,321 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:40,182 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '3f03381c-c140-48e2-b237-b16ef2ac2824', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:39 GMT', 'content-type': 'application/json', 'content-length': '43437', 'connection': 'keep-alive', 'x-amzn-requestid': '3f03381c-c140-48e2-b237-b16ef2ac2824', 'x-amzn-bedrock-invocation-latency': '71', 'x-amzn-bedrock-input-token-count': '16'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:47,332 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '3802f211-445a-420c-baeb-a3a8771d5712', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:46 GMT', 'content-type': 'application/json', 'content-length': '43319', 'connection': 'keep-alive', 'x-amzn-requestid': '3802f211-445a-420c-baeb-a3a8771d5712', 'x-amzn-bedrock-invocation-latency': '77', 'x-amzn-bedrock-input-token-count': '18'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:57,241 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'e57190bb-3c8b-44e6-a303-ede74e0e4056', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:49 GMT', 'content-type': 'application/json', 'content-length': '43411', 'connection': 'keep-alive', 'x-amzn-requestid': 'e57190bb-3c8b-44e6-a303-ede74e0e4056', 'x-amzn-bedrock-invocation-latency': '80', 'x-amzn-bedrock-input-token-count': '22'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:59,861 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '0e918a66-b293-4568-81f8-ef233ece74bf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:59 GMT', 'content-type': 'application/json', 'content-length': '43411', 'connection': 'keep-alive', 'x-amzn-requestid': '0e918a66-b293-4568-81f8-ef233ece74bf', 'x-amzn-bedrock-invocation-latency': '73', 'x-amzn-bedrock-input-token-count': '22'}, 'RetryAttempts': 0}\n",
      "Evaluating:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [05:30<00:34, 34.06s/it]2025-12-06 16:42:05,011 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [05:36<00:00, 37.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèÜ OFFICIAL RAGAS SCORECARD\n",
      "==================================================\n",
      "{'faithfulness': 1.0000, 'answer_relevancy': 0.9556, 'context_precision': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: (Optional) Re-Run Ragas Metrics\n",
    "# Note: This takes 1-2 minutes and costs a small amount of API usage.\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.run_config import RunConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Prepare Data from the results you just generated\n",
    "questions_list = [row['Question'] for row in results]\n",
    "answers_list = [row['AI Answer'] for row in results]\n",
    "ground_truths = [\n",
    "    \"Normal cracking, settlement of new structures, movement of made up ground, coastal erosion, defective design.\",\n",
    "    \"The document does not state a specific deductible for 'Personal Property', only for Jewellery and Portables.\",\n",
    "    \"Temporary or permanent dispossession by government order, or unlawful occupation by any person.\"\n",
    "]\n",
    "\n",
    "# We need to fetch the contexts again manually for Ragas\n",
    "contexts_list = []\n",
    "print(\"fetching contexts for evaluation...\")\n",
    "for q in questions_list:\n",
    "    # Quick re-fetch of the text the LLM saw\n",
    "    retrieved_text = intelligent_retrieval(q)\n",
    "    contexts_list.append([retrieved_text])\n",
    "\n",
    "data_samples = {\n",
    "    \"question\": questions_list,\n",
    "    \"answer\": answers_list,\n",
    "    \"contexts\": contexts_list,\n",
    "    \"ground_truth\": ground_truths\n",
    "}\n",
    "\n",
    "ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# 2. Configure Ragas with Safety Mode (Sequential)\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "safe_config = RunConfig(max_workers=1, timeout=120, max_retries=3)\n",
    "\n",
    "# 3. Run\n",
    "print(\"üë®‚Äç‚öñÔ∏è Calculating Final Scores...\")\n",
    "eval_results = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    "    run_config=safe_config,\n",
    "    raise_exceptions=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÜ OFFICIAL RAGAS SCORECARD\")\n",
    "print(\"=\"*50)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344400c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pashdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ee206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2723926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\pashdoc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded. Environment verified.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & Environment Setup\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain & AI Libraries\n",
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_aws import BedrockEmbeddings, ChatBedrock\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "# Pinecone & Evaluation\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "# from ragas import evaluate\n",
    "# from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "# from ragas.run_config import RunConfig\n",
    "# from datasets import Dataset\n",
    "\n",
    "# Load Environment Variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded. Environment verified.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a98390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangSmith Connected!\n",
      "   Project: sbi-insurance-rag-project\n",
      "   API Key Set: Yes\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.5: Initialize LangSmith Tracing\n",
    "import langsmith\n",
    "from langsmith import Client\n",
    "\n",
    "# Explicitly enable tracing (this ensures it's active in notebooks)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"sbi-insurance-rag-project\"\n",
    "\n",
    "# Verify LangSmith is configured\n",
    "try:\n",
    "    ls_client = Client()\n",
    "    print(f\"‚úÖ LangSmith Connected!\")\n",
    "    print(f\"   Project: {os.getenv('LANGCHAIN_PROJECT')}\")\n",
    "    print(f\"   API Key Set: {'Yes' if os.getenv('LANGCHAIN_API_KEY') else 'No'}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è LangSmith Connection Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c78b159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index 'sbi-home-insurance-rag-hybrid' already exists.\n",
      "üîç Checking if 'SBIhomeinsurance_home.pdf' is already in the database...\n",
      "‚úÖ File 'SBIhomeinsurance_home.pdf' detected in Pinecone.\n",
      "üöÄ SKIPPING Docling & Embeddings to save cost.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Smart Initialization & Duplicate Check\n",
    "# Configuration\n",
    "file_path = \"SBIhomeinsurance_home.pdf\" # Make sure this matches your file name\n",
    "index_name = \"sbi-home-insurance-rag-hybrid\" # Using your existing hybrid index name\n",
    "\n",
    "# 1. Connect to Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# 2. Check if Index Exists\n",
    "existing_indexes = [index.name for index in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    print(f\"‚ö†Ô∏è Index '{index_name}' not found. Creating it...\")\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024, # Titan v2\n",
    "        metric=\"dotproduct\", # Required for Hybrid\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    time.sleep(20) # Wait for init\n",
    "    print(\"‚úÖ Index created successfully.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Index '{index_name}' already exists.\")\n",
    "\n",
    "# 3. Connect to the Index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# 4. Check if File is Already Ingested (The \"Smart\" Check)\n",
    "# We perform a dummy query filtering by this specific source file\n",
    "print(f\"üîç Checking if '{file_path}' is already in the database...\")\n",
    "\n",
    "# We use a dummy vector just to trigger the metadata filter\n",
    "dummy_vector = [0.0] * 1024 \n",
    "check_response = index.query(\n",
    "    vector=dummy_vector,\n",
    "    top_k=1,\n",
    "    filter={\"source\": file_path},\n",
    "    include_metadata=False\n",
    ")\n",
    "\n",
    "if len(check_response['matches']) > 0:\n",
    "    print(f\"‚úÖ File '{file_path}' detected in Pinecone.\")\n",
    "    print(\"üöÄ SKIPPING Docling & Embeddings to save cost.\")\n",
    "    should_ingest = False\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è File '{file_path}' NOT found in Pinecone.\")\n",
    "    print(\"‚öôÔ∏è Proceeding with Ingestion...\")\n",
    "    should_ingest = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87a8f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipping Loading & Chunking (Data already exists).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load & Chunk (Conditional)\n",
    "final_chunks = []\n",
    "\n",
    "if should_ingest:\n",
    "    print(f\"üìÑ Starting Docling processing for {file_path}...\")\n",
    "    \n",
    "    # A. Load with Docling (Export to Markdown)\n",
    "    loader = DoclingLoader(\n",
    "        file_path=file_path,\n",
    "        export_type=ExportType.MARKDOWN\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(\"‚úÖ PDF Loaded via Docling.\")\n",
    "\n",
    "    # B. Split by Headers (Level 1)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    md_header_splits = markdown_splitter.split_text(docs[0].page_content)\n",
    "    \n",
    "    # C. Split by Size (Level 2)\n",
    "    chunk_size = 1000\n",
    "    chunk_overlap = 200\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    final_chunks = text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "    # D. Add Metadata Tags (Crucial for Smart Indexing)\n",
    "    for chunk in final_chunks:\n",
    "        chunk.metadata[\"source\"] = file_path # Used for filtering later\n",
    "        # We also keep the 'text' in metadata for Hybrid retrieval\n",
    "        chunk.metadata[\"text\"] = chunk.page_content \n",
    "    \n",
    "    print(f\"‚úÖ Chunking Complete. Created {len(final_chunks)} chunks.\")\n",
    "    print(\"Sample Metadata:\", final_chunks[0].metadata)\n",
    "\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping Loading & Chunking (Data already exists).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1cb347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060b3122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 00:39:55,579 - INFO - Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Skipped Ingestion. Loaded existing BM25 params from file.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Hybrid Embedding & Upsert (Conditional)\n",
    "import boto3\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "# 1. Initialize AWS Bedrock Embeddings (Need this for both Ingestion AND Querying)\n",
    "boto3_session = boto3.Session()\n",
    "bedrock_client = boto3_session.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    "    client=bedrock_client\n",
    ")\n",
    "\n",
    "# 2. Initialize BM25 Encoder\n",
    "bm25 = BM25Encoder()\n",
    "bm25_filename = \"bm25_values.json\"\n",
    "\n",
    "if should_ingest:\n",
    "    print(\"‚öôÔ∏è Generatings Embeddings & Upserting...\")\n",
    "    \n",
    "    # A. Fit BM25 on the new text\n",
    "    chunk_texts = [chunk.page_content for chunk in final_chunks]\n",
    "    bm25.fit(chunk_texts)\n",
    "    bm25.dump(bm25_filename) # Save for future use\n",
    "    print(\"‚úÖ BM25 Encoder fitted and saved.\")\n",
    "    \n",
    "    # B. Generate Vectors & Upsert\n",
    "    vectors_to_upsert = []\n",
    "    \n",
    "    print(f\"Generating vectors for {len(final_chunks)} chunks...\")\n",
    "    for i, chunk in enumerate(final_chunks):\n",
    "        # 1. Dense Vector (Titan)\n",
    "        dense_vec = embeddings.embed_query(chunk.page_content)\n",
    "        \n",
    "        # 2. Sparse Vector (BM25)\n",
    "        sparse_vec = bm25.encode_documents(chunk.page_content)\n",
    "        \n",
    "        # 3. Create ID (Unique based on source + index)\n",
    "        # We use a simple hash or index. Here index 'i' is fine for this run.\n",
    "        # Ideally, hash the text to avoid dupes, but for now:\n",
    "        vector_id = f\"{file_path}_{i}\"\n",
    "        \n",
    "        vectors_to_upsert.append({\n",
    "            \"id\": vector_id,\n",
    "            \"values\": dense_vec,\n",
    "            \"sparse_values\": sparse_vec,\n",
    "            \"metadata\": chunk.metadata # Includes 'source' and 'text'\n",
    "        })\n",
    "        \n",
    "    # C. Batch Upsert to Pinecone\n",
    "    batch_size = 50\n",
    "    for i in range(0, len(vectors_to_upsert), batch_size):\n",
    "        batch = vectors_to_upsert[i : i + batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "        print(f\"   Uploaded batch {i} to {i+batch_size}\")\n",
    "        \n",
    "    print(\"‚úÖ Ingestion Complete.\")\n",
    "\n",
    "else:\n",
    "    # If we skipped ingestion, we MUST load the BM25 model from disk\n",
    "    # so we can still run queries.\n",
    "    if os.path.exists(bm25_filename):\n",
    "        bm25.load(bm25_filename)\n",
    "        print(\"‚úÖ Skipped Ingestion. Loaded existing BM25 params from file.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: BM25 file not found. You might need to re-ingest if retrieval fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b45b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 00:40:04,697 - INFO - Found credentials in environment variables.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cohere Re-ranker Initialized.\n",
      "‚úÖ Retrieval Logic Defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Setup Retrieval & Re-ranking Engines\n",
    "from typing import List\n",
    "\n",
    "# 1. Define the Bedrock Cohere Re-ranker Class\n",
    "class BedrockCohereReranker:\n",
    "    def __init__(self, region_name=\"us-east-1\"):\n",
    "        self.client = boto3.client(\"bedrock-runtime\", region_name=region_name)\n",
    "        self.model_id = \"cohere.rerank-v3-5:0\"\n",
    "\n",
    "    def rerank(self, query: str, docs: List[str], top_n: int = 5):\n",
    "        # Docs must be a list of strings for the API\n",
    "        if not docs: return []\n",
    "        \n",
    "        request_body = {\n",
    "            \"query\": query, \n",
    "            \"documents\": docs, \n",
    "            \"top_n\": top_n, \n",
    "            \"api_version\": 2\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.client.invoke_model(modelId=self.model_id, body=json.dumps(request_body))\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            results = response_body.get(\"results\", [])\n",
    "            return results # Returns list of {'index': int, 'relevance_score': float}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Rerank Error: {e}\")\n",
    "            # Fallback: return indices 0..top_n\n",
    "            return [{\"index\": i, \"relevance_score\": 0.0} for i in range(min(len(docs), top_n))]\n",
    "\n",
    "# Initialize the Reranker\n",
    "reranker = BedrockCohereReranker()\n",
    "print(\"‚úÖ Cohere Re-ranker Initialized.\")\n",
    "\n",
    "# 2. Define the \"Intelligent Retrieval\" Function\n",
    "# This combines Hybrid Search (Pinecone) + Re-ranking (Cohere)\n",
    "def intelligent_retrieval(query: str) -> str:\n",
    "    print(f\"üîé Searching for: '{query}'\")\n",
    "    \n",
    "    # A. Hybrid Search in Pinecone (Top 25)\n",
    "    dense_vec = embeddings.embed_query(query)\n",
    "    # Note: If you want strict keyword matching, enable the line below:\n",
    "    # sparse_vec = bm25.encode_queries(query) \n",
    "    \n",
    "    results = index.query(\n",
    "        vector=dense_vec,\n",
    "        # sparse_vector=sparse_vec, # Uncomment if passing sparse values\n",
    "        top_k=25,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Extract just the text from the matches\n",
    "    raw_docs = [match['metadata']['text'] for match in results['matches']]\n",
    "    \n",
    "    if not raw_docs:\n",
    "        return \"\"\n",
    "\n",
    "    # B. Re-ranking (Filter 25 -> Top 5)\n",
    "    rerank_results = reranker.rerank(query, raw_docs, top_n=5)\n",
    "    \n",
    "    # C. Format the Top 5 for the LLM\n",
    "    top_docs_text = []\n",
    "    for res in rerank_results:\n",
    "        idx = res['index']\n",
    "        top_docs_text.append(raw_docs[idx])\n",
    "        \n",
    "    return \"\\n\\n\".join(top_docs_text)\n",
    "\n",
    "print(\"‚úÖ Retrieval Logic Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b027279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ anthropic.claude-3-5-haiku Model Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5.5: Initialize anthropic.claude-3-5-haiku Model\n",
    "\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# We use the US Cross-Region Inference Profile for Llama 3.1\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\",  ## us.meta.llama3-1-70b-instruct-v1:0\n",
    "    client=bedrock_client, # We defined this client in Cell 4\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_tokens\": 512} # max_tokens\": 2048\n",
    ")\n",
    "\n",
    "print(\"‚úÖ anthropic.claude-3-5-haiku Model Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f35d0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10ff8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Chain (Production Ready) Created.\n",
      "\n",
      "üß™ Sanity Check Query: 'what is the name of company which provides this insurance? and give me address for this company. also give me contact details for this comapny'\n",
      "----------------------------------------\n",
      "üîé Searching for: 'what is the name of company which provides this insurance? and give me address for this company. also give me contact details for this comapny'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 00:40:13,543 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '1ccc8b58-6690-4b49-b94e-85a04d244916', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:10:13 GMT', 'content-type': 'application/json', 'content-length': '43421', 'connection': 'keep-alive', 'x-amzn-requestid': '1ccc8b58-6690-4b49-b94e-85a04d244916', 'x-amzn-bedrock-invocation-latency': '87', 'x-amzn-bedrock-input-token-count': '30'}, 'RetryAttempts': 0}\n",
      "2025-12-11 00:40:16,104 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, here are the details:\n",
      "\n",
      "Company Name: SBI General Insurance Company Limited\n",
      "\n",
      "Address: \n",
      "Fulcrum Building, 9th Floor, A & B Wing, Sahar Road, Andheri (East), Mumbai - 400099\n",
      "\n",
      "Contact Details:\n",
      "- Toll-free Number: 18001021111\n",
      "- Email: customer.care@sbigeneral.in\n",
      "- Website: www.sbigeneral.in\n",
      "\n",
      "Additional Information:\n",
      "- CIN (Corporate Identification Number): U66000MH2009PLC190546\n",
      "- IRDAI Registration Number: 144\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: LLM Chain Setup\n",
    "# 1. Define the Prompt\n",
    "# We strictly tell the LLM to use ONLY the provided context.\n",
    "prompt_template = \"\"\"\n",
    "You are an expert Insurance Assistant. Use the following pieces of retrieved context to answer the question.\n",
    "If the answer is not in the context, just say that you don't know. Do not try to make up an answer.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 2. Define the Chain\n",
    "# This pipeline does: Take Query -> Get Smart Context -> Format Prompt -> Run Llama 3 -> Parse String\n",
    "rag_chain_final = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(intelligent_retrieval), # Uses our Hybrid + Rerank function\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain (Production Ready) Created.\")\n",
    "\n",
    "# 3. Quick Sanity Check\n",
    "# Let's run a simple test to make sure the chain flows correctly\n",
    "\n",
    "test_q = \"what is the name of company which provides this insurance? and give me address for this company. also give me contact details for this comapny\"\n",
    "#\"What specific exclusions apply to loss caused by Subsidence?\"\n",
    "#\"What is the deductible for Personal Property?\"\n",
    "#\"from the document tell me in terms of payment what policys provide how much insurance back means in terms of money\"\n",
    "\n",
    "\n",
    "print(f\"\\nüß™ Sanity Check Query: '{test_q}'\")\n",
    "print(\"-\" * 40)\n",
    "print(rag_chain_final.invoke(test_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0483fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Sanity Check Query: 'What specific exclusions apply to loss caused by Subsidence?'\n",
      "----------------------------------------\n",
      "üîé Searching for: 'What specific exclusions apply to loss caused by Subsidence?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 00:46:09,667 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'bc7e0f54-25ba-445b-9c4f-54eae07090c8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:16:09 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': 'bc7e0f54-25ba-445b-9c4f-54eae07090c8', 'x-amzn-bedrock-invocation-latency': '86', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-11 00:46:11,836 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, for subsidence, the following exclusions apply:\n",
      "\n",
      "Subsidence is excluded when caused by:\n",
      "a. Normal cracking, settlement or bedding down of new structures\n",
      "b. The settlement or movement of made up ground\n",
      "c. Coastal or river erosion\n",
      "d. Defective design or workmanship or use of defective materials\n",
      "e. Demolition, construction, structural alterations or repair of any property\n",
      "f. Groundworks or excavations\n",
      "\n",
      "These exclusions are specifically listed under section 6 of the context, which covers \"Subsidence of the land on which Your Home Buildings stands, Landslide, Rockslide\".\n"
     ]
    }
   ],
   "source": [
    "# In Cell 6, at the bottom - RUN THIS AGAIN after restart\n",
    "test_q = \"What specific exclusions apply to loss caused by Subsidence?\"\n",
    "\n",
    "print(f\"\\nüß™ Sanity Check Query: '{test_q}'\")\n",
    "print(\"-\" * 40)\n",
    "result = rag_chain_final.invoke(test_q)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4874afb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Sanity Check Query: 'who is prasad?'\n",
      "----------------------------------------\n",
      "üîé Searching for: 'who is prasad?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 00:58:38,341 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'c278da52-5e15-41e2-836c-9af1521079a1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:28:38 GMT', 'content-type': 'application/json', 'content-length': '43336', 'connection': 'keep-alive', 'x-amzn-requestid': 'c278da52-5e15-41e2-836c-9af1521079a1', 'x-amzn-bedrock-invocation-latency': '88', 'x-amzn-bedrock-input-token-count': '6'}, 'RetryAttempts': 0}\n",
      "2025-12-11 00:58:40,460 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I cannot find any specific information about who Prasad is. The context contains definitions related to insurance terminology like medical practitioners, mis-selling, nominees, and policy holders, but there is no mention of a person named Prasad. Therefore, I do not know who Prasad is from this context.\n"
     ]
    }
   ],
   "source": [
    "# In Cell 6, at the bottom - RUN THIS AGAIN after restart\n",
    "test_q = \"who is prasad?\"\n",
    "\n",
    "print(f\"\\nüß™ Sanity Check Query: '{test_q}'\")\n",
    "print(\"-\" * 40)\n",
    "result = rag_chain_final.invoke(test_q)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Property` not found.\n",
      "\n",
      "üß™ Sanity Check Query: 'who is prasad?'\n",
      "----------------------------------------\n",
      "üîé Searching for: 'who is prasad?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:05:35,309 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '15260c9f-c11f-43af-9edd-3d79b6d22024', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:35:35 GMT', 'content-type': 'application/json', 'content-length': '43336', 'connection': 'keep-alive', 'x-amzn-requestid': '15260c9f-c11f-43af-9edd-3d79b6d22024', 'x-amzn-bedrock-invocation-latency': '75', 'x-amzn-bedrock-input-token-count': '6'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:05:37,388 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I cannot find any specific information about who Prasad is. The context contains definitions related to insurance terminology like medical practitioners, mis-selling, nominees, and policy holders, but there is no mention of a person named Prasad. Therefore, I do not know who Prasad is from this context.\n"
     ]
    }
   ],
   "source": [
    "# In Cell 6, at the bottom - RUN THIS AGAIN after restart\n",
    "test_q = 'who is prasad?\n",
    "\n",
    "print(f\"\\nüß™ Sanity Check Query: '{test_q}'\")\n",
    "print(\"-\" * 40)\n",
    "result = rag_chain_final.invoke(test_q)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fdc20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a36ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Creating new dataset: 'sbi-insurance-qa-eval'...\n",
      "üìù Adding 15 examples to dataset...\n",
      "‚úÖ Dataset created successfully with 15 examples!\n",
      "\n",
      "üîó View your dataset: https://smith.langchain.com/datasets/cdbbfeab-3c59-4a87-b828-a2425c4d0512\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create Evaluation Dataset for RAG (Based on Actual SBI Policy)\n",
    "from langsmith import Client\n",
    "\n",
    "# Initialize LangSmith client\n",
    "ls_client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"sbi-insurance-qa-eval\"\n",
    "\n",
    "# Evaluation Examples with REAL questions and answers from the PDF\n",
    "eval_examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What specific exclusions apply to loss caused by Subsidence?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"Subsidence losses are excluded if caused by: (a) normal cracking, settlement or bedding down of new structures, (b) settlement or movement of made up ground, (c) coastal or river erosion, (d) defective design or workmanship or use of defective materials, or demolition, construction, structural alterations or repair of any property, or groundworks or excavations\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the compensation amount for Personal Accident death cover?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"The Personal Accident Cover provides compensation of Rs 5,00,000 (Five Lakh) per person in the event of unfortunate death of either the policyholder or their spouse due to an insured peril\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Are acts of terrorism covered under this policy?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"Yes, acts of terrorism are covered as per the Terrorism Clause attached to the policy, with applicable exclusions and excess mentioned in that clause\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How much time do I have to submit a claim after noticing loss or damage?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"You must submit your claim in the claim form at the earliest opportunity, but within 30 days from the date you first notice the loss or damage\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Is earthquake damage covered as standard?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"Yes, earthquake, volcanic eruption, or other convulsions of nature are covered perils under Section 1 Fire and Allied Perils of the policy\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the maximum period for Loss of Rent coverage?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"The Loss of Rent coverage is available for the reasonable time required to repair the Home Building to make it fit for living. The maximum period of this cover is three years from the date the Home Building becomes unfit for living\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Are electrical appliances covered for breakdown?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"Electrical/electronic damage by over-running, short circuiting, arcing, self-heating or leakage of electricity is excluded under Section 1. However, Section 6 Breakdown of Domestic Electric Electronic Appliances provides optional coverage for unforeseen mechanical/electrical breakdown and accidental external damage\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What happens to the policy if my home remains unoccupied?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"Under the Burglary and Theft section, the policy will not make payment if loss or damage occurs while your home is Unoccupied (more than 30 consecutive days) unless the company was informed at the time of applying for insurance or prior to the home being unoccupied, and it is signified by an endorsement on the policy\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the built-in cover for Home Contents if I purchase Home Building cover?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"The policy has a built-in cover for General Contents equal to 20% of the Sum Insured for Home Building Cover, subject to a maximum of Rs 10 Lakh, provided both Home Building and Home Contents cover are opted for\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the sum insured restoration process after a claim?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"After payment of any loss, the policy shall be restored to the full original amount of Sum Insured. The policyholder must pay proportionate premium for the unexpired Policy Period from the date of loss, which can be deducted from the net claim amount\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What reports must be submitted to authorities after fire damage?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"In case of fire, explosion, implosion or lightning damage, you must give immediate report to the fire brigade of the local authority and the police\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is covered under Burglary section for newly purchased contents?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"Newly purchased Contents purchased after commencement of the Policy are covered subject to maximum payment of 10% of the Section Sum Insured or Rs 20,000 whichever is less, duly supported by original purchase invoice/bill\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is the Ambulance Expenses benefit limit?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"Ambulance Expenses benefit provides reimbursement up to Rs 5,000 per policy per year for reasonable and customary expenses incurred towards transportation by a registered ambulance service provider to a Hospital in case of an Emergency requiring hospitalization\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What are the architect and debris removal cost limits?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"The policy pays up to 5% of the claim amount for reasonable fees of architect, surveyor, consulting engineer and up to 2% of the claim amount for reasonable costs of removing debris from the site\"}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Are pre-existing diseases covered under Personal Accident section?\"},\n",
    "        \"outputs\": {\"expected_answer\": \"No, any Pre-existing Disease or Disability arising out of Pre-existing Diseases or any complication arising therefrom is specifically excluded under the Personal Accident section\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create or Check if Dataset Exists\n",
    "try:\n",
    "    dataset = ls_client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\"‚úÖ Dataset '{dataset_name}' already exists with {len(list(ls_client.list_examples(dataset_id=dataset.id)))} examples.\")\n",
    "except:\n",
    "    print(f\"‚öôÔ∏è Creating new dataset: '{dataset_name}'...\")\n",
    "    dataset = ls_client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"Evaluation dataset for SBI Home Insurance RAG with 15 realistic questions and accurate ground truth answers from policy document\"\n",
    "    )\n",
    "    \n",
    "    print(f\"üìù Adding {len(eval_examples)} examples to dataset...\")\n",
    "    for example in eval_examples:\n",
    "        ls_client.create_example(\n",
    "            inputs=example[\"inputs\"],\n",
    "            outputs=example[\"outputs\"],\n",
    "            dataset_id=dataset.id\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Dataset created successfully with {len(eval_examples)} examples!\")\n",
    "\n",
    "print(f\"\\nüîó View your dataset: https://smith.langchain.com/datasets/{dataset.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d8e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90813954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Starting evaluation on dataset...\n",
      "This will run your RAG chain on all 15 questions and track results in LangSmith.\n",
      "\n",
      "View the evaluation results for experiment: 'sbi-rag-eval-3fb11671' at:\n",
      "https://smith.langchain.com/o/361d1350-8788-45f2-83b5-4450425627e0/datasets/cdbbfeab-3c59-4a87-b828-a2425c4d0512/compare?selectedSessions=77099f6f-31ed-4079-8fcf-f76faf1e0cf9\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'Are pre-existing diseases covered under Personal Accident section?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:26:25,134 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'bd3d2b89-77db-4a25-a8e5-4d98a2098c9f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:56:25 GMT', 'content-type': 'application/json', 'content-length': '43377', 'connection': 'keep-alive', 'x-amzn-requestid': 'bd3d2b89-77db-4a25-a8e5-4d98a2098c9f', 'x-amzn-bedrock-invocation-latency': '80', 'x-amzn-bedrock-input-token-count': '11'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:26:27,369 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What are the architect and debris removal cost limits?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:26:30,465 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '609e3259-7a26-45fc-8091-bb9a85dedbb1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:56:30 GMT', 'content-type': 'application/json', 'content-length': '43355', 'connection': 'keep-alive', 'x-amzn-requestid': '609e3259-7a26-45fc-8091-bb9a85dedbb1', 'x-amzn-bedrock-invocation-latency': '80', 'x-amzn-bedrock-input-token-count': '11'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:26:31,228 - INFO - Using Bedrock Invoke API to generate response\n",
      "1it [00:08,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is the Ambulance Expenses benefit limit?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:26:33,854 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '2361ddd2-a9c1-4068-8d9d-b74551473fb3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:56:34 GMT', 'content-type': 'application/json', 'content-length': '43335', 'connection': 'keep-alive', 'x-amzn-requestid': '2361ddd2-a9c1-4068-8d9d-b74551473fb3', 'x-amzn-bedrock-invocation-latency': '78', 'x-amzn-bedrock-input-token-count': '10'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:26:34,569 - INFO - Using Bedrock Invoke API to generate response\n",
      "2it [00:12,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is covered under Burglary section for newly purchased contents?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:26:37,715 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '4a9ad05b-857b-46e3-b06e-aa14c52d5300', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:56:37 GMT', 'content-type': 'application/json', 'content-length': '43365', 'connection': 'keep-alive', 'x-amzn-requestid': '4a9ad05b-857b-46e3-b06e-aa14c52d5300', 'x-amzn-bedrock-invocation-latency': '71', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:26:47,159 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rerank Error: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:25,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What reports must be submitted to authorities after fire damage?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:26:50,725 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'dfb5541b-0e05-4b68-b5ed-0a563670acfe', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:56:50 GMT', 'content-type': 'application/json', 'content-length': '43385', 'connection': 'keep-alive', 'x-amzn-requestid': 'dfb5541b-0e05-4b68-b5ed-0a563670acfe', 'x-amzn-bedrock-invocation-latency': '85', 'x-amzn-bedrock-input-token-count': '12'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:26:51,454 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is the sum insured restoration process after a claim?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:26:55,610 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'ed5faafd-f003-472a-9914-0e7a2c2defed', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:56:55 GMT', 'content-type': 'application/json', 'content-length': '43403', 'connection': 'keep-alive', 'x-amzn-requestid': 'ed5faafd-f003-472a-9914-0e7a2c2defed', 'x-amzn-bedrock-invocation-latency': '91', 'x-amzn-bedrock-input-token-count': '12'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:27:04,455 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rerank Error: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:44,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is the built-in cover for Home Contents if I purchase Home Building cover?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:27:09,859 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '2ca44ddd-d476-45cf-b1d6-17a4c8984e39', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:57:10 GMT', 'content-type': 'application/json', 'content-length': '43317', 'connection': 'keep-alive', 'x-amzn-requestid': '2ca44ddd-d476-45cf-b1d6-17a4c8984e39', 'x-amzn-bedrock-invocation-latency': '81', 'x-amzn-bedrock-input-token-count': '17'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:27:10,554 - INFO - Using Bedrock Invoke API to generate response\n",
      "6it [00:48,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What happens to the policy if my home remains unoccupied?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:27:13,315 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'c9c9c2ac-26c9-4ffc-95fb-2f994e4f972c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:57:13 GMT', 'content-type': 'application/json', 'content-length': '43435', 'connection': 'keep-alive', 'x-amzn-requestid': 'c9c9c2ac-26c9-4ffc-95fb-2f994e4f972c', 'x-amzn-bedrock-invocation-latency': '79', 'x-amzn-bedrock-input-token-count': '13'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:27:18,155 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rerank Error: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:56,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'Are electrical appliances covered for breakdown?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:27:21,679 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'b5569a11-26c6-45b3-9ef1-952726bb7dbc', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:57:21 GMT', 'content-type': 'application/json', 'content-length': '43324', 'connection': 'keep-alive', 'x-amzn-requestid': 'b5569a11-26c6-45b3-9ef1-952726bb7dbc', 'x-amzn-bedrock-invocation-latency': '87', 'x-amzn-bedrock-input-token-count': '8'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:27:28,054 - INFO - Using Bedrock Invoke API to generate response\n",
      "8it [01:07,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is the maximum period for Loss of Rent coverage?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:27:32,134 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'abb17ecb-0f61-445e-87e0-75b5272b63f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:57:32 GMT', 'content-type': 'application/json', 'content-length': '43403', 'connection': 'keep-alive', 'x-amzn-requestid': 'abb17ecb-0f61-445e-87e0-75b5272b63f2', 'x-amzn-bedrock-invocation-latency': '80', 'x-amzn-bedrock-input-token-count': '12'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:27:46,279 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rerank Error: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:23, 10.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'Is earthquake damage covered as standard?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:27:48,594 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '009f87ff-233d-4ce8-a451-a6682adfd887', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:57:48 GMT', 'content-type': 'application/json', 'content-length': '43294', 'connection': 'keep-alive', 'x-amzn-requestid': '009f87ff-233d-4ce8-a451-a6682adfd887', 'x-amzn-bedrock-invocation-latency': '73', 'x-amzn-bedrock-input-token-count': '8'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:27:49,309 - INFO - Using Bedrock Invoke API to generate response\n",
      "10it [01:26,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'How much time do I have to submit a claim after noticing loss or damage?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:27:52,094 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '834e5451-bfce-4c2e-9cd8-63aad103aff0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:57:52 GMT', 'content-type': 'application/json', 'content-length': '43311', 'connection': 'keep-alive', 'x-amzn-requestid': '834e5451-bfce-4c2e-9cd8-63aad103aff0', 'x-amzn-bedrock-invocation-latency': '84', 'x-amzn-bedrock-input-token-count': '17'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:28:01,145 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rerank Error: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:39,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'Are acts of terrorism covered under this policy?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:28:04,919 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'fcffc10e-7ad0-429f-859c-c35996b38f1c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:58:05 GMT', 'content-type': 'application/json', 'content-length': '43345', 'connection': 'keep-alive', 'x-amzn-requestid': 'fcffc10e-7ad0-429f-859c-c35996b38f1c', 'x-amzn-bedrock-invocation-latency': '75', 'x-amzn-bedrock-input-token-count': '10'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:28:09,493 - INFO - Using Bedrock Invoke API to generate response\n",
      "12it [01:48,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is the compensation amount for Personal Accident death cover?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:28:13,204 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'ed69308f-895a-4970-8bee-dc8ff766959c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:58:13 GMT', 'content-type': 'application/json', 'content-length': '43287', 'connection': 'keep-alive', 'x-amzn-requestid': 'ed69308f-895a-4970-8bee-dc8ff766959c', 'x-amzn-bedrock-invocation-latency': '77', 'x-amzn-bedrock-input-token-count': '12'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:28:27,058 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Rerank Error: An error occurred (ThrottlingException) when calling the InvokeModel operation (reached max retries: 4): Too many requests, please wait before trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [02:06, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What specific exclusions apply to loss caused by Subsidence?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 01:28:31,096 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'e5d65ed1-9e42-4cfd-8745-5924f6913a22', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 10 Dec 2025 19:58:31 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': 'e5d65ed1-9e42-4cfd-8745-5924f6913a22', 'x-amzn-bedrock-invocation-latency': '75', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-11 01:28:31,779 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-11 01:28:32,330 - WARNING - Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('422 Client Error: unknown for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Unprocessable entity: error reading multipart data: multipart: NextPart: EOF\"}\\n')\n",
      "15it [02:10,  8.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation Complete!\n",
      "üìä Results Summary:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ExperimentResults' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ Evaluation Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Results Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Total Questions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexample_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Answers Provided: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_provided\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m{})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîó View detailed results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mexperiment_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ExperimentResults' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "# Cell 8: Run Evaluation on Dataset\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# 1. Define a wrapper function for your RAG chain\n",
    "# LangSmith evaluate() expects a function that takes a dict with \"question\" \n",
    "# and returns a dict with the answer\n",
    "def rag_predict(inputs: dict) -> dict:\n",
    "    \"\"\"Wrapper function for RAG chain evaluation\"\"\"\n",
    "    question = inputs[\"question\"]\n",
    "    answer = rag_chain_final.invoke(question)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# 2. Define a simple evaluator\n",
    "# This checks if the answer is relevant (not empty and not \"I don't know\")\n",
    "def answer_not_empty(run, example):\n",
    "    \"\"\"Check if RAG provided a meaningful answer\"\"\"\n",
    "    answer = run.outputs.get(\"answer\", \"\").lower()\n",
    "    \n",
    "    # Check if answer is meaningful\n",
    "    is_meaningful = (\n",
    "        len(answer) > 20 and \n",
    "        \"don't know\" not in answer and\n",
    "        answer.strip() != \"\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"answer_provided\",\n",
    "        \"score\": 1 if is_meaningful else 0\n",
    "    }\n",
    "\n",
    "# 3. Run the evaluation\n",
    "print(\"üß™ Starting evaluation on dataset...\")\n",
    "print(\"This will run your RAG chain on all 15 questions and track results in LangSmith.\\n\")\n",
    "\n",
    "results = evaluate(\n",
    "    rag_predict,  # Your RAG function\n",
    "    data=dataset_name,  # The dataset we just created\n",
    "    evaluators=[answer_not_empty],  # Basic evaluator\n",
    "    experiment_prefix=\"sbi-rag-eval\",  # Name for this evaluation run\n",
    "    description=\"Testing RAG system with hybrid search + reranking on SBI insurance questions\",\n",
    "    max_concurrency=1  # Run one at a time to avoid API limits\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation Complete!\")\n",
    "print(f\"üìä Results Summary:\")\n",
    "print(f\"   - Total Questions: {results.summary['example_count']}\")\n",
    "print(f\"   - Answers Provided: {results.summary.get('answer_provided', {}).get('score', 0) * 100:.1f}%\")\n",
    "print(f\"\\nüîó View detailed results: {results.experiment_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc857363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e53e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ee86db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e5b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7512f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e78e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffe70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 (Updated): Setup Retrieval & Re-ranking with Tracing\n",
    "\n",
    "## this cell for to check the context/chunk recive to re-ranker in Langsmith\n",
    "\n",
    "from typing import List\n",
    "from langsmith import traceable\n",
    "\n",
    "# 1. Define the Bedrock Cohere Re-ranker Class\n",
    "class BedrockCohereReranker:\n",
    "    def __init__(self, region_name=\"us-east-1\"):\n",
    "        self.client = boto3.client(\"bedrock-runtime\", region_name=region_name)\n",
    "        self.model_id = \"cohere.rerank-v3-5:0\"\n",
    "\n",
    "    @traceable(name=\"Cohere Reranker\")  # This makes it visible in LangSmith!\n",
    "    def rerank(self, query: str, docs: List[str], top_n: int = 5):\n",
    "        \"\"\"Rerank documents using Cohere Rerank v3.5\"\"\"\n",
    "        if not docs: \n",
    "            return []\n",
    "        \n",
    "        request_body = {\n",
    "            \"query\": query, \n",
    "            \"documents\": docs, \n",
    "            \"top_n\": top_n, \n",
    "            \"api_version\": 2\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = self.client.invoke_model(\n",
    "                modelId=self.model_id, \n",
    "                body=json.dumps(request_body)\n",
    "            )\n",
    "            response_body = json.loads(response['body'].read())\n",
    "            results = response_body.get(\"results\", [])\n",
    "            \n",
    "            # Log metadata for LangSmith\n",
    "            print(f\"‚úÖ Reranked {len(docs)} -> {len(results)} docs\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Rerank Error: {e}\")\n",
    "            # Fallback: return indices 0..top_n\n",
    "            return [{\"index\": i, \"relevance_score\": 0.0} for i in range(min(len(docs), top_n))]\n",
    "\n",
    "\n",
    "# Initialize the Reranker\n",
    "reranker = BedrockCohereReranker()\n",
    "print(\"‚úÖ Cohere Re-ranker Initialized.\")\n",
    "\n",
    "\n",
    "# 2. Define the \"Intelligent Retrieval\" Function with Tracing\n",
    "@traceable(name=\"Hybrid Retrieval + Reranking\")  # This wraps the entire retrieval process!\n",
    "def intelligent_retrieval(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs hybrid search in Pinecone followed by Cohere reranking.\n",
    "    Returns top 5 most relevant document chunks.\n",
    "    \"\"\"\n",
    "    print(f\"üîé Searching for: '{query}'\")\n",
    "    \n",
    "    # A. Hybrid Search in Pinecone (Top 25)\n",
    "    dense_vec = embeddings.embed_query(query)\n",
    "    \n",
    "    results = index.query(\n",
    "        vector=dense_vec,\n",
    "        top_k=25,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    # Extract text from matches\n",
    "    raw_docs = [match['metadata']['text'] for match in results['matches']]\n",
    "    \n",
    "    if not raw_docs:\n",
    "        return \"\"\n",
    "\n",
    "    # B. Re-ranking (Filter 25 -> Top 5)\n",
    "    # The @traceable decorator on rerank() will show this step in LangSmith\n",
    "    rerank_results = reranker.rerank(query, raw_docs, top_n=5)\n",
    "    \n",
    "    # C. Format the Top 5 for the LLM\n",
    "    top_docs_text = []\n",
    "    for res in rerank_results:\n",
    "        idx = res['index']\n",
    "        top_docs_text.append(raw_docs[idx])\n",
    "        \n",
    "    return \"\\n\\n\".join(top_docs_text)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Retrieval Logic Defined with Tracing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test to see tracing\n",
    "test_q = \"What are the architect and debris removal cost limits?\"\n",
    "result = rag_chain_final.invoke(test_q)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf89ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce40b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982ee8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94131bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running Final Evaluation on Test Set...\n",
      "--------------------------------------------------\n",
      "Asking: What specific exclusions apply to loss caused by Subsidence?\n",
      "üîé Searching for: 'What specific exclusions apply to loss caused by Subsidence?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:30:42,721 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '80e646a4-7d00-42c3-b10f-33d176fed6c3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:00:23 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': '80e646a4-7d00-42c3-b10f-33d176fed6c3', 'x-amzn-bedrock-invocation-latency': '95', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:31:17,781 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking: What is the deductible for Personal Property?\n",
      "üîé Searching for: 'What is the deductible for Personal Property?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:31:27,586 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '0574e333-07bf-451a-a7a8-964937c2869c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:01:22 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': '0574e333-07bf-451a-a7a8-964937c2869c', 'x-amzn-bedrock-invocation-latency': '78', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:31:39,071 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asking: What are the specific exclusions for Riot, strikes, or malicious damages?\n",
      "üîé Searching for: 'What are the specific exclusions for Riot, strikes, or malicious damages?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:31:51,091 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '5f507c34-7f0a-4b1f-896b-f02b37e2ddee', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:01:42 GMT', 'content-type': 'application/json', 'content-length': '43437', 'connection': 'keep-alive', 'x-amzn-requestid': '5f507c34-7f0a-4b1f-896b-f02b37e2ddee', 'x-amzn-bedrock-invocation-latency': '73', 'x-amzn-bedrock-input-token-count': '16'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:32:01,671 - INFO - Using Bedrock Invoke API to generate response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üèÜ FINAL PROJECT ACCURACY REPORT\n",
      "============================================================\n",
      "\n",
      "Q1: What specific exclusions apply to loss caused by Subsidence?\n",
      "A: According to the context, the specific exclusions that apply to loss caused by Subsidence of the land on which the home building stands are:\n",
      "\n",
      "a. normal cracking, settlement or bedding down of new structures,\n",
      "b. the settlement or movement of made up ground,\n",
      "c. coastal or river erosion,\n",
      "d. defective design or workmanship or use of defective materials, or demolition, construction, structural alterations or repair of any property, or groundworks or excavations.\n",
      "----------------------------------------\n",
      "\n",
      "Q2: What is the deductible for Personal Property?\n",
      "A: The deductible for Personal Property is not explicitly mentioned in the context. However, it does mention deductibles for Jewellery & Valuables (5% of the claim amount subject to a minimum of Rs 2500) and portable equipment's (5% of claim amount subject to a minimum of Rs 1000).\n",
      "----------------------------------------\n",
      "\n",
      "Q3: What are the specific exclusions for Riot, strikes, or malicious damages?\n",
      "A: According to the context, the specific exclusions for Riot, Strikes, or Malicious Damages are:\n",
      "\n",
      "* Caused by temporary or permanent dispossession, confiscation, commandeering, requisition or destruction by order of the government or any lawful authority.\n",
      "* Caused by temporary or permanent dispossession of Your Home by unlawful occupation by any person.\n",
      "\n",
      "(Section 10 of the context)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Final Evaluation Run\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the Hard Questions\n",
    "test_questions = [\n",
    "    \"What specific exclusions apply to loss caused by Subsidence?\", \n",
    "    \"What is the deductible for Personal Property?\",\n",
    "    \"What are the specific exclusions for Riot, strikes, or malicious damages?\"\n",
    "]\n",
    "\n",
    "print(\"üöÄ Running Final Evaluation on Test Set...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "results = []\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"Asking: {q}\")\n",
    "    try:\n",
    "        # Run the robust chain\n",
    "        answer = rag_chain_final.invoke(q)\n",
    "        \n",
    "        # Save result\n",
    "        results.append({\n",
    "            \"Question\": q,\n",
    "            \"AI Answer\": answer.strip(),\n",
    "            \"Status\": \"‚úÖ Success\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        results.append({\n",
    "            \"Question\": q,\n",
    "            \"AI Answer\": f\"ERROR: {e}\",\n",
    "            \"Status\": \"‚ùå Failed\"\n",
    "        })\n",
    "\n",
    "# 2. Display Results in a Clean Table\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üèÜ FINAL PROJECT ACCURACY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print full details for verification\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"\\nQ{i+1}: {row['Question']}\")\n",
    "    print(f\"A: {row['AI Answer']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a367fc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580cb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching contexts for evaluation...\n",
      "üîé Searching for: 'What specific exclusions apply to loss caused by Subsidence?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:36:10,511 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '9bbd495c-b7a9-4f53-8227-d8358966bec3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:05:29 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': '9bbd495c-b7a9-4f53-8227-d8358966bec3', 'x-amzn-bedrock-invocation-latency': '79', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What is the deductible for Personal Property?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:36:20,926 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '7df64f13-5668-44b7-aadc-8bc414676274', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:06:17 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': '7df64f13-5668-44b7-aadc-8bc414676274', 'x-amzn-bedrock-invocation-latency': '84', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Searching for: 'What are the specific exclusions for Riot, strikes, or malicious damages?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 16:36:27,671 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '45222c46-ecb4-40e1-b43b-e07e1dcd5813', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:06:26 GMT', 'content-type': 'application/json', 'content-length': '43437', 'connection': 'keep-alive', 'x-amzn-requestid': '45222c46-ecb4-40e1-b43b-e07e1dcd5813', 'x-amzn-bedrock-invocation-latency': '72', 'x-amzn-bedrock-input-token-count': '16'}, 'RetryAttempts': 0}\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1600\\1291553097.py:38: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  ragas_llm = LangchainLLMWrapper(llm)\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_1600\\1291553097.py:39: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üë®‚Äç‚öñÔ∏è Calculating Final Scores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]2025-12-06 16:36:34,386 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:36:53,511 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:13,642 - ERROR - Exception raised in Job[0]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
      "Evaluating:  11%|‚ñà         | 1/9 [00:39<05:16, 39.51s/it]2025-12-06 16:37:13,691 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:13,696 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:13,701 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:37:42,161 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '05eef54a-6587-4f86-a5e2-7e66d57a2745', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:07:41 GMT', 'content-type': 'application/json', 'content-length': '43375', 'connection': 'keep-alive', 'x-amzn-requestid': '05eef54a-6587-4f86-a5e2-7e66d57a2745', 'x-amzn-bedrock-invocation-latency': '75', 'x-amzn-bedrock-input-token-count': '14'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:38:43,652 - ERROR - Error raised by inference endpoint\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 892, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1017, in _raw_read\n",
      "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1000, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 482, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 631, in _safe_read\n",
      "    data = self.fp.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1307, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1163, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 98, in read\n",
      "    chunk = self._raw_stream.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1101, in read\n",
      "    data = self._raw_read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1016, in _raw_read\n",
      "    with self._error_catcher():\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 897, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\n",
      "urllib3.exceptions.ReadTimeoutError: AWSHTTPSConnectionPool(host='bedrock-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\langchain_aws\\embeddings\\bedrock.py\", line 258, in _invoke_model\n",
      "    response_body = json.loads(response.get(\"body\").read())\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 101, in read\n",
      "    raise ReadTimeoutError(endpoint_url=e.url, error=e)\n",
      "botocore.exceptions.ReadTimeoutError: Read timeout on endpoint URL: \"None\"\n",
      "2025-12-06 16:38:43,951 - ERROR - Exception raised in Job[1]: ReadTimeoutError(Read timeout on endpoint URL: \"None\")\n",
      "Evaluating:  22%|‚ñà‚ñà‚ñè       | 2/9 [02:09<08:05, 69.39s/it]2025-12-06 16:38:43,961 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [02:16<04:04, 40.72s/it]2025-12-06 16:38:50,561 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:05,641 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:17,491 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [02:59<03:27, 41.49s/it]2025-12-06 16:39:33,251 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:33,261 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:33,266 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:39:43,061 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '65b1c2ee-9dfb-4b30-b0af-c2870038a07b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:09:38 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': '65b1c2ee-9dfb-4b30-b0af-c2870038a07b', 'x-amzn-bedrock-invocation-latency': '79', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:40:02,221 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'dd4367a8-f380-451a-9d8a-0a4540faa367', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:09:56 GMT', 'content-type': 'application/json', 'content-length': '43328', 'connection': 'keep-alive', 'x-amzn-requestid': 'dd4367a8-f380-451a-9d8a-0a4540faa367', 'x-amzn-bedrock-invocation-latency': '73', 'x-amzn-bedrock-input-token-count': '9'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:02,231 - ERROR - Error raised by inference endpoint\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 892, in _error_catcher\n",
      "    yield\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1017, in _raw_read\n",
      "    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1000, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 482, in read\n",
      "    s = self._safe_read(self.length)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\http\\client.py\", line 631, in _safe_read\n",
      "    data = self.fp.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\socket.py\", line 717, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1307, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\ssl.py\", line 1163, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "TimeoutError: The read operation timed out\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 98, in read\n",
      "    chunk = self._raw_stream.read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1101, in read\n",
      "    data = self._raw_read(amt)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 1016, in _raw_read\n",
      "    with self._error_catcher():\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\contextlib.py\", line 153, in __exit__\n",
      "    self.gen.throw(typ, value, traceback)\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\urllib3\\response.py\", line 897, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\") from e  # type: ignore[arg-type]\n",
      "urllib3.exceptions.ReadTimeoutError: AWSHTTPSConnectionPool(host='bedrock-runtime.us-east-1.amazonaws.com', port=443): Read timed out.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\langchain_aws\\embeddings\\bedrock.py\", line 258, in _invoke_model\n",
      "    response_body = json.loads(response.get(\"body\").read())\n",
      "  File \"c:\\Anaconda3\\envs\\doc\\lib\\site-packages\\botocore\\response.py\", line 101, in read\n",
      "    raise ReadTimeoutError(endpoint_url=e.url, error=e)\n",
      "botocore.exceptions.ReadTimeoutError: Read timeout on endpoint URL: \"None\"\n",
      "2025-12-06 16:41:02,231 - ERROR - Exception raised in Job[4]: ReadTimeoutError(Read timeout on endpoint URL: \"None\")\n",
      "Evaluating:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [04:28<03:54, 58.63s/it]2025-12-06 16:41:02,256 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [04:35<02:03, 41.31s/it]2025-12-06 16:41:09,961 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:20,156 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [05:01<01:12, 36.10s/it]2025-12-06 16:41:35,311 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:35,311 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:35,321 - INFO - Using Bedrock Invoke API to generate response\n",
      "2025-12-06 16:41:40,182 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '3f03381c-c140-48e2-b237-b16ef2ac2824', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:39 GMT', 'content-type': 'application/json', 'content-length': '43437', 'connection': 'keep-alive', 'x-amzn-requestid': '3f03381c-c140-48e2-b237-b16ef2ac2824', 'x-amzn-bedrock-invocation-latency': '71', 'x-amzn-bedrock-input-token-count': '16'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:47,332 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '3802f211-445a-420c-baeb-a3a8771d5712', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:46 GMT', 'content-type': 'application/json', 'content-length': '43319', 'connection': 'keep-alive', 'x-amzn-requestid': '3802f211-445a-420c-baeb-a3a8771d5712', 'x-amzn-bedrock-invocation-latency': '77', 'x-amzn-bedrock-input-token-count': '18'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:57,241 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': 'e57190bb-3c8b-44e6-a303-ede74e0e4056', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:49 GMT', 'content-type': 'application/json', 'content-length': '43411', 'connection': 'keep-alive', 'x-amzn-requestid': 'e57190bb-3c8b-44e6-a303-ede74e0e4056', 'x-amzn-bedrock-invocation-latency': '80', 'x-amzn-bedrock-input-token-count': '22'}, 'RetryAttempts': 0}\n",
      "2025-12-06 16:41:59,861 - INFO - Successfully invoked model amazon.titan-embed-text-v2:0. ResponseMetadata: {'RequestId': '0e918a66-b293-4568-81f8-ef233ece74bf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Sat, 06 Dec 2025 11:11:59 GMT', 'content-type': 'application/json', 'content-length': '43411', 'connection': 'keep-alive', 'x-amzn-requestid': '0e918a66-b293-4568-81f8-ef233ece74bf', 'x-amzn-bedrock-invocation-latency': '73', 'x-amzn-bedrock-input-token-count': '22'}, 'RetryAttempts': 0}\n",
      "Evaluating:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [05:30<00:34, 34.06s/it]2025-12-06 16:42:05,011 - INFO - Using Bedrock Invoke API to generate response\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [05:36<00:00, 37.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üèÜ OFFICIAL RAGAS SCORECARD\n",
      "==================================================\n",
      "{'faithfulness': 1.0000, 'answer_relevancy': 0.9556, 'context_precision': 1.0000}\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: (Optional) Re-Run Ragas Metrics\n",
    "# Note: This takes 1-2 minutes and costs a small amount of API usage.\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.run_config import RunConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# 1. Prepare Data from the results you just generated\n",
    "questions_list = [row['Question'] for row in results]\n",
    "answers_list = [row['AI Answer'] for row in results]\n",
    "ground_truths = [\n",
    "    \"Normal cracking, settlement of new structures, movement of made up ground, coastal erosion, defective design.\",\n",
    "    \"The document does not state a specific deductible for 'Personal Property', only for Jewellery and Portables.\",\n",
    "    \"Temporary or permanent dispossession by government order, or unlawful occupation by any person.\"\n",
    "]\n",
    "\n",
    "# We need to fetch the contexts again manually for Ragas\n",
    "contexts_list = []\n",
    "print(\"fetching contexts for evaluation...\")\n",
    "for q in questions_list:\n",
    "    # Quick re-fetch of the text the LLM saw\n",
    "    retrieved_text = intelligent_retrieval(q)\n",
    "    contexts_list.append([retrieved_text])\n",
    "\n",
    "data_samples = {\n",
    "    \"question\": questions_list,\n",
    "    \"answer\": answers_list,\n",
    "    \"contexts\": contexts_list,\n",
    "    \"ground_truth\": ground_truths\n",
    "}\n",
    "\n",
    "ragas_dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# 2. Configure Ragas with Safety Mode (Sequential)\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "safe_config = RunConfig(max_workers=1, timeout=120, max_retries=3)\n",
    "\n",
    "# 3. Run\n",
    "print(\"üë®‚Äç‚öñÔ∏è Calculating Final Scores...\")\n",
    "eval_results = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, context_precision],\n",
    "    llm=ragas_llm,\n",
    "    embeddings=ragas_embeddings,\n",
    "    run_config=safe_config,\n",
    "    raise_exceptions=False\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üèÜ OFFICIAL RAGAS SCORECARD\")\n",
    "print(\"=\"*50)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2344400c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pashdoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
